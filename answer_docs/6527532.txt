<ol>
<li><p>Each year there are two or three
isolated blog posts that claim to
compare performance and size of one
or two programs written in different
languages.</p>

<p><em>As a resource</em> the blog posts fail for obvious
reasons, most obviously:</p>

<ul>
<li><p>not updated with newer versions of the language implementation</p></li>
<li><p>not updated with better programs</p></li>
</ul></li>
<li><p>Every couple of years someone
dissatisfied with something about
the benchmarks game (often some
detail about the code repository or
website technology) starts a project that will
fix everything they dislike about the benchmarks game.</p>

<p><em>As a resource</em> the most obvious problem with those
projects is that they never seem to get
close to publishing performance
data.</p></li>
<li><p>Every year some group of programmers
campaigns to have language X
included in the benchmarks game,
while some other group demands that
some program is included (or
excluded).</p>

<p>Sadly, they rarely accept that among
the <em>resources</em> provided by the
benchmarks game are </p>

<ul>
<li><p><a href="http://benchmarksgame.alioth.debian.org/play.php#languagex" rel="nofollow">scripts they can use</a> to make and publish language performance
measurements</p></li>
<li><p>examples of which basic information (language version, build
commands, run commands, measurement
techniques, ...) is required to provide context for the measurements.</p></li>
</ul>

<p>They rarely accept that they are
empowered to create what they wish
to see.</p></li>
</ol>