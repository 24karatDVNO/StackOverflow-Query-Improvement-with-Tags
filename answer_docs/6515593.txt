<p>It is well known that <a href="http://en.wikipedia.org/wiki/LU_decomposition#Algorithms" rel="nofollow">LU decomposition</a> (equiv. to the row reduction phase of Gauss elimination without pivoting) of a full NxN matrix A requires ~2NÂ³/3 multiplications and divisions.  Even with partial pivoting this operation count holds.</p>

<p>Floating point arithmetic can entail rounding errors at every step.  "Forward" error analysis of the elimination steps doesn't produce a useful estimate of accuracy, because in the worst case rounding errors can accumulate exponentially.  However J.H. Wilkinson showed <a href="http://galton.uchicago.edu/~lekheng/courses/302/gauss-jordan2.pdf" rel="nofollow">"backward error analysis</a> can give realistic estimates, e.g. for A positive definite or diagonally dominant matrices, in which the computed solution by Gauss elimination with full pivoting is the exact solution of a "modest" perturbation of the system to be solved (usually the case as well with just partial pivoting).  The size of the residual error can then be estimated from the size of the perturbation and the condition number of the matrix, usually defined as the ratio of the norm of A to that of A's inverse.  If A is singular, this is infinite, and if A is close to singular, then arbitrarily large.  If the condition number is large enough to amplify the perturbation size (normally the machine epsilon of floating point operations) into greater-than-acceptable residual errors, we say the matrix is ill-conditioned.  Otherwise we say A is well-conditioned.</p>

<p>Of course an idea would be to avoid the rounding errors of floating point arithmetic by using instead exact arithmetic, either integer or rational.</p>

<p>But exact integer-preserving arithmetic generally leads to rapid growth of entries and hence overflow, even if the matrix is well-conditioned in the above sense.  <a href="http://www.ams.org/journals/mcom/1968-22-103/S0025-5718-1968-0226829-0/S0025-5718-1968-0226829-0.pdf" rel="nofollow">Various strategies</a> for minimizing the growth of entries have been proposed, going back to Jordan (whose name is often linked with Gauss in the version of elimination that computes a matrix inverse instead of the solution of a linear system).  W. Kahan gives <a href="http://www.cs.berkeley.edu/~wkahan/MathH110/chio.pdf" rel="nofollow">a particularly concise account</a>.  An arbitrary precision integer (or rational) implementation would address this difficulty.</p>

<p>However it seems doubtful that such exact methods could compete with floating point arithmetic on dense matrices.  If a direct method like Gauss elimination does not achieve the desired accuracy, as checked by computing the residual (multiplying matrix A times the computed solution and subtracting it from the vector of right-hand sides), then solving again for a correction term with a linear system with the same matrix A but right-hand sides corresponding to the residual.  If the reduction phase of Gauss elimination was actually done as an LU factorization, then only the backsolve phase is needed to solve for iterative corrections.</p>

<p>Where the best accuracy must be squeezed from the available floating point precision, direct methods based on orthogonal matrices are useful (see <a href="http://en.wikipedia.org/wiki/Householder_transformation" rel="nofollow">Householder</a> and <a href="http://en.wikipedia.org/wiki/Givens_rotation" rel="nofollow">Givens</a> transforms).</p>

<p>The bottom line is that solution of linear systems is <a href="http://www.netlib.org/utk/people/JackDongarra/la-sw.html" rel="nofollow">an oft-reinvented wheel</a>, and a stronger case for software reuse can scarcely be imagined.  See <a href="http://www.stanford.edu/class/ee392o/nlas-foils.pdf" rel="nofollow">the third slide of this presentation</a>: "How to write numerical linear algebra software -- DON'T!  Whenever possible, rely on existing, mature software libraries for performing numerical linear algebra computations."</p>